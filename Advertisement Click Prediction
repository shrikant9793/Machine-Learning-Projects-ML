Model Information 
We are using ten different classification algorithms -

* K-Nearest Neighbours Algorithm : K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique. K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories. K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.
* Support Vector Machine Algorithm : Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as            Regression problems. However, primarily, it is used for Classification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.
* Random Forest Classifier : Random Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can be used for both Classification and     Regression problems in ML. It is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the     performance of the model.
* Logistic Regression : Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex   extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).
* Naive Bayes Classifcation Algorithm : Naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem with strong independence         assumptions between the features. They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve higher accuracy levels.
* Decision Tree Classifier : Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for     	 solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each       leaf node represents the outcome.
* Gradient Boosting Algorithm : Gradient boosting is a machine learning technique for regression, classification and other tasks, which produces a prediction model in the form of   an ensemble of weak prediction models, typically decision trees.
* AdaBoosting Algorithm : AdaBoost, short for Adaptive Boosting, is a statistical classification meta-algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003       GÃ¶del Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance.
* Artificial Neural Network : Artificial neural networks, usually simply called neural networks, are computing systems vaguely inspired by the biological neural networks that       constitute animal brains. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain.
* XgBoost Classifier : XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured   data (images, text, etc.) A wide range of applications: Can be used to solve regression, classification, ranking, and user-defined prediction problems.
